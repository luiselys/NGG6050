{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiselys/NGG605public/blob/main/Copy_of_Frequentist_Versus_Bayesian_Approaches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRG_xXWNpwj3"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PennNGG/Quantitative-Neuroscience/blob/master/Concepts/Python/Frequentist%20Versus%20Bayesian%20Approaches.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKIiY6p3GRFq"
      },
      "source": [
        "# Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7VmLUr5GTNw"
      },
      "source": [
        "Debates between frequentists and Bayesians have carried on for years, touching on issues that are in some cases very [practical](https://www.ejwagenmakers.com/2008/BayesFreqBook.pdf) and other cases much more [philosophical](http://www.stat.columbia.edu/~gelman/research/published/philosophy.pdf). The goal here is not to dive deeply into all of those debates but rather to introduce you to the basic issues, because they are at the heart of what we can and cannot do with statistics.\n",
        "\n",
        "More specifically, the two camps differ fundamentally on how to interpret randomness, which profoundly affects the kinds of inferences that can be drawn on the basis of noisy data:\n",
        "\n",
        "A **frequentist** thinks of probability only in terms of the frequency of many repeated events that include some element of randomness. **To a frequentist, assigning a probability to a singular event that can either happen or not happen, particularly one that is not directly or yet measured,  is nonsensical** (\"There is no place in our system for speculations concerning the probability that the sun will rise tomorrow\" -- William Feller). As a consequence of these ideas, a frequentist operates on the conditional distribution of the data, assuming that a hypothesis is true. That is, one makes a series of repeated measurements (the data) under fixed conditions, obtaining what is essentially a histogram. Inferences about the nature of the process that generated the data then allow only for this definition of randomness or uncertainty: the obtained variability in the data. Questions of the form \"What is the probability that process x generated my data?\" are undefined in this framework, because a probability cannot be assigned to an unknown and unseeable process (or \"hypothesis\"), only to repeated measures. Instead, the best you can do is simply assume that a particular process was the one that generated your data, and then ask \"What is the probability that I would have obtained my data, assuming that x was the true process?\" This question is the basis for null hypotheses (typically defined in terms of the parameters of the probability distribution that you would expect the data to be drawn from under a particular set of assumptions) and p-values: computing the likelihood p(data | null hypothesis).\n",
        "\n",
        "Benefits of this approach are that frequentist-based statistics are typically relatively easy to compute, they require few assumptions, and they tend to promote good experimental design (because you need to very carefully control the conditions under which the data are collected).\n",
        "\n",
        "Drawbacks include the fact that definitions of probability in this framework are often highly counter-intuitive to how most of us think, resulting in results that can be very difficult to interpret. A good example is the concept of a \"confidence interval\" in frequentist statistics, which is described nicely [here](https://jfiksel.github.io/2018-01-08-explaining-confidence-intervals/).\n",
        "\n",
        "A **Bayesian** thinks of probability as the degree of belief in a proposition (or hypothesis). In this framework, data represent evidence that can support or oppose such a belief, which is represented as a  probability distribution. Thus, unlike from the frequentist perspective, **from the Bayesian perspective it is perfectly natural to describe the belief (or probability) that particular values of particular parameters of a particular probability distribution (together encompassing a \"hypothesis\" about the data) are true**.\n",
        "\n",
        "These ideas are derived directly from the definition of joint probability (see [Independence and Lack Thereof](https://colab.research.google.com/drive/1YFwKKkWUjtV6_Nx2upNpFYHJNeXIeQB6) for a related discussion):\n",
        "\n",
        "$P(A\\cap B)=p(A|B)\\times p(B) = p(B|A)\\times p(A)$\n",
        "\n",
        "where $P(A\\cap B)$ is read as \"the probability that A and B are true\" and P(A | B) is read as \"the probability that A is true, given that B is true\" or just \"the probability of A given B.\"\n",
        "\n",
        "If we call A the Hypothesis and B the Data, and rearrange, we get Bayes' Rule:\n",
        "\n",
        "$P(Hypothesis|Data)=\\frac{P(Data|hypothesis)\\times P(Hypothesis)}{P(Data)}$\n",
        "\n",
        "Where *P*(*Hypothesis* | *Data*) is called the posterior probability (or just posterior), *P*(*Data* | *Hypothesis*) is the likelihood, *P*(*Hypothesis*) is the prior, and *P*(*Data*) is the marginal probability of the data.\n",
        "\n",
        "Benefits of the Bayesian approach are that it tends to get at the intuitive concepts that one is addressing (e.g., the probability that a hypothesis is true, given the data), and it does so in a rigorous manner.\n",
        "\n",
        "Drawbacks include questions about [how to identify an appropriate prior](https://stats.stackexchange.com/questions/78606/how-to-choose-prior-in-bayesian-parameter-estimation).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isP38xJSbJuA"
      },
      "source": [
        "# Tutorial and Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Eh2Pu8glb6C"
      },
      "source": [
        "To use this tutorial, read the text and then try to generate code to solve the exercises. Answers will be posted to GitHub after the class they are due.\n",
        "\n",
        "The learning objective is to gain insights into thinking about inference from a \"Frequentist\" versus a \"Bayesian\" perspective. In brief, because a Frequentist does not consider the probability of an event or state of the world or hypothesis, only their frequency of occurrance, it is not possible to ask questions of the form \"what is the probabilty that hypothesis x is true?\" Instead, one can only consider questions of the form, \"what is the probabilty that I would have obtained my data, given that hypothesis x is true?\" In contrast, Bayesians consider the probabilities of such things (often called the strength of belief), but doing so can require making assumptions that can be difficult to prove.\n",
        "\n",
        "Let's start with a simple example, taken from:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Base_rate_fallacy#Example_1:_HIV\n",
        "\n",
        "\"Imagine running an HIV test on A SAMPLE of 1000 persons ...\"\n",
        "\n",
        "\"The test has a false positive rate of 5% (0.05)...\" i.e., the probability that someone who takes the test gets a POSITIVE result despite the fact that the person does NOT have HIV\n",
        "\n",
        "\"...and no false negative rate.\" i.e., The probability that someone who takes the test gets a NEGATIVE result despite the fact that the person DOES have HIV.\n",
        "\n",
        "Answers to the exercises below will be found [here](https://github.com/PennNGG/Quantitative-Neuroscience/tree/master/Answers%20to%20Exercises/Python) after the due date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3hkaaTFlukb"
      },
      "source": [
        "### Exercise #1: If someone gets a positive test, is it \"statistically significant\" at the p<0.05 level? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, it is not statistically significanct because we have to account for the false positive rate. PPV = TP/(TP+FP) and varies with prevalence. Prevalence is affected by incidence (infection rate). High infection rate increases prevalence which increases the PPV."
      ],
      "metadata": {
        "id": "Dg_wq9AEp0zF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ItR0WXCmBgL"
      },
      "source": [
        "### Exercise #2: What is the probability that if someone gets a positive test, that person is infected?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ7aAEoKmkTV"
      },
      "source": [
        "Following on Exercise #2, let's do the same thing, but this time we will try different values for the proportion of the population that is actually infected. What you should notice is that the **PROPORTION INFECTED GIVEN A POSITIVE TEST** depends (a lot!) on the **OVERALL RATE OF INFECTION**. Put another way, to determine the probabilty of a hypothesis, given your data (e.g., proportion infected given a positive test), you have to know the probability that the hypothesis was true without any data.\n",
        "\n",
        "Why is this the case? It is a simple consequence of the definition of a conditional probability, formulated as Bayes' Rule. Specifically, the joint probability of two events, call them A and B, is defined as: $$p(A\\,and\\,B) = p(A) \\times p(B\\,|\\,A)$$ $$p(B\\,and\\,A) = p(B) \\times p(A\\,|\\,B)$$\n",
        "\n",
        "Now, calling A the Hypothesis and B the Data, then rearranging, we get:$$p(Hypothesis\\,|\\,Data) = \\frac{p(Data\\,|\\,Hypothesis) \\times p(Hypothesis)}{p(Data)}$$\n",
        "\n",
        "So you cannot calculate the probability of the hypothesis, given the data (i.e., the Bayesian posterior), without knowing the probability of the hypothesis independent of any data (i.e., the prior).\n",
        "\n",
        "For this exercise, assume a range of priors (infection rates) from 0 to 1 in steps of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Define the range of priors (infection rates)\n",
        "priors = np.arange(0, 1.1, 0.1)\n",
        "\n",
        "# Define the sample size\n",
        "sample_size = 1000\n",
        "\n",
        "# Define the false positive rate\n",
        "false_positive_rate = 0.05\n",
        "\n",
        "# Define the likelihood function (Binomial distribution)\n",
        "def likelihood(prior):\n",
        "    # Probability of a positive test result given the hypothesis (HIV infection rate)\n",
        "    p_positive_given_hypothesis = prior * (1 - false_positive_rate)  # No false negatives\n",
        "\n",
        "    # Calculate the likelihood using Binomial distribution\n",
        "    likelihood = binom.pmf(k=sample_size, n=sample_size, p=p_positive_given_hypothesis)\n",
        "\n",
        "    return likelihood\n",
        "\n",
        "# Calculate the unnormalized and normalized posteriors for each prior\n",
        "posteriors = []\n",
        "normalized_posteriors = []\n",
        "for prior in priors:\n",
        "    likelihood_value = likelihood(prior)\n",
        "\n",
        "    # Calculate unnormalized posterior\n",
        "    posterior = likelihood_value * prior\n",
        "\n",
        "    posteriors.append(posterior)\n",
        "\n",
        "    # Normalize and append normalized posterior\n",
        "    normalized_posterior = posterior / np.sum(posteriors)\n",
        "    normalized_posteriors.append(normalized_posterior)\n",
        "\n",
        "# Create a table with priors, unnormalized posteriors, and normalized posteriors using Pandas\n",
        "table = pd.DataFrame({'Priors': priors, 'Unnormalized Posteriors': posteriors, 'Normalized Posteriors': normalized_posteriors})\n",
        "\n",
        "# Print the table\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-pGnf23taAF",
        "outputId": "5a09e9ff-6ac9-4a33-bcf0-dfccf762424c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Priors  Unnormalized Posteriors  Normalized Posteriors\n",
            "0      0.0             0.000000e+00                    NaN\n",
            "1      0.1             0.000000e+00                    NaN\n",
            "2      0.2             0.000000e+00                    NaN\n",
            "3      0.3             0.000000e+00                    NaN\n",
            "4      0.4             0.000000e+00                    NaN\n",
            "5      0.5             0.000000e+00                    NaN\n",
            "6      0.6            4.497870e-245                    1.0\n",
            "7      0.7            4.642408e-178                    1.0\n",
            "8      0.8            5.208135e-120                    1.0\n",
            "9      0.9             8.324482e-69                    1.0\n",
            "10     1.0             5.291823e-23                    1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-b6eece1e7f05>:36: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  normalized_posterior = posterior / np.sum(posteriors)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "p(D given hypothesis) is the likelihood which in this case is based on the infection rate and the fact that we are assuming no false negatives.\n",
        "p(hypothesis) is the prior (infection rates) which we independently changing.\n",
        "p(data) is the evidence."
      ],
      "metadata": {
        "id": "HsCVGcU-q2fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import binom\n",
        "\n",
        "# Define the range of priors (infection rates)\n",
        "priors = np.arange(0, 1.1, 0.1)\n",
        "\n",
        "# Define the sample size\n",
        "sample_size = 1000\n",
        "\n",
        "# Define the false positive rate\n",
        "false_positive_rate = 0.05\n",
        "\n",
        "# Define the likelihood function (Binomial distribution)\n",
        "def likelihood(prior):\n",
        "    # Probability of a positive test result given the hypothesis (HIV infection rate)\n",
        "    p_positive_given_hypothesis = prior * (1 - false_positive_rate)  # No false negatives\n",
        "\n",
        "    # Calculate the likelihood using Binomial distribution\n",
        "    likelihood = binom.pmf(k=sample_size, n=sample_size, p=p_positive_given_hypothesis)\n",
        "\n",
        "    return likelihood\n",
        "\n",
        "# Calculate the unnormalized posterior for each prior\n",
        "posteriors = []\n",
        "for prior in priors:\n",
        "    likelihood_value = likelihood(prior)\n",
        "\n",
        "    # Calculate unnormalized posterior\n",
        "    posterior = likelihood_value * prior\n",
        "\n",
        "    posteriors.append(posterior)\n",
        "\n",
        "# Normalize the posterior probabilities\n",
        "posteriors = np.array(posteriors)\n",
        "normalized_posteriors = posteriors / np.sum(posteriors)\n",
        "\n",
        "# Create a table with priors and normalized posteriors using Pandas\n",
        "table = pd.DataFrame({'Priors': priors, 'Normalized Posteriors': normalized_posteriors})\n",
        "\n",
        "# Print the table\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcpMCji3q15_",
        "outputId": "83a2d54e-0a52-49f5-8fb5-aae1d081ff6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Priors  Normalized Posteriors\n",
            "0      0.0           0.000000e+00\n",
            "1      0.1           0.000000e+00\n",
            "2      0.2           0.000000e+00\n",
            "3      0.3           0.000000e+00\n",
            "4      0.4           0.000000e+00\n",
            "5      0.5           0.000000e+00\n",
            "6      0.6          8.499662e-223\n",
            "7      0.7          8.772796e-156\n",
            "8      0.8           9.841855e-98\n",
            "9      0.9           1.573084e-46\n",
            "10     1.0           1.000000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tteEm2Qlgbb3"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Copyright 2021 by Joshua I. Gold, University of Pennsylvania"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pKIiY6p3GRFq",
        "J3hkaaTFlukb"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}